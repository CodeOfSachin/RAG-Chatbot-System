{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"NzClv1NSvp0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","projectPath = '/content/drive/MyDrive/RAG_Chatbot_System'\n","os.chdir(projectPath)"],"metadata":{"id":"Lyuhdyqf7ler"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Z7Ay3ZqGtUZP"},"outputs":[],"source":["# Required packages\n","!pip install -U sentence-transformers chromadb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kF06A6MhD8MD"},"outputs":[],"source":["!pip install transformers torch huggingface-hub -qqq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCpP6Zl321Jh"},"outputs":[],"source":["!pip install -q accelerate bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUMusi96tcTQ"},"outputs":[],"source":["# Import Required packages\n","# import config  # Token Key\n","import pandas as pd\n","import numpy as np\n","import chromadb\n","import re\n","import logging\n","import torch\n","import textwrap\n","from sentence_transformers import SentenceTransformer\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedTokenizerBase\n","from typing import Dict, Any, List, Union, Optional\n","from huggingface_hub import login\n","import hashlib\n","import pickle\n","\n","login(token = config.huggingFaceToken) # Your Hugging Face Login Token\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9m0YQXqtwy5"},"outputs":[],"source":["# Load the CSV file\n","def load_csv(fileName: str = 'tnt.csv', encoding: Optional[str] = 'utf-8') -> pd.DataFrame:\n","  \"\"\" Load a CSV file into a pandas DataFrame.\n","  Args:\n","      fileName (str, optional): Path to the CSV file. Defaults to 'tnt.csv'.\n","      encoding (str, None, optional): Encoding of the CSV file. Defaults to 'utf-8'.\n","\n","  Returns:\n","      pd.DataFrame: The loaded DataFrame.\n","\n","  Raises:\n","      FileNotFoundError: If the specified file does not exist.\n","      pd.errors.ParserError: If the CSV file is malformed.\n","      UnicodeDecodeError: If the file cannot be decoded with the specified encoding.\n","      Exception: For other unexpected errors during CSV loading.\n","  \"\"\"\n","  try:\n","    df = pd.read_csv(fileName, encoding=encoding)\n","  except FileNotFoundError:\n","    logger.error(f\"Error: File '{fileName}' not found. Please ensure the file exists in the working directory.\")\n","    raise\n","  except pd.errors.ParserError as e:\n","    logger.error(f\"Error: Failed to parse CSV file '{fileName}'. The file may be malformed: {e}\")\n","    raise\n","  except UnicodeDecodeError as e:\n","    logger.error(f\"Error: Unable to decode file '{fileName}' with encoding '{encoding}': {e}\")\n","    raise\n","  except Exception as e:\n","    logger.error(f\"Error: An unexpected error occurred while loading '{fileName}': {e}\")\n","    raise\n","  else:\n","    logger.info(f\"Successfully loaded the file: {fileName}\")\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L41pJxGXt1lw"},"outputs":[],"source":["#Extract numeric amount for price or discount\n","def extract_numeric_amount(number: Union[str, float, int, None]) -> Optional[float]:\n","\n","  \"\"\" Convert number strings or numbers to numeric float values.\n","\n","  Handles strings with commas, alphabet letters (e.g., '9% off'), currency symbols, or decimal points (e.g., '$1,500.99')\n","  and numeric inputs (e.g., 1500, 1500.0). Returns None for invalid or missing inputs.\n","\n","  Args:\n","      number (str, float, int, None): Input number, which can be a string (e.g., '1,500', '$1,500.99', '9% off'),\n","      a number (e.g., 1500, 1500.0), or None/Nan.\n","\n","  Returns:\n","      Optional[float]: The cleaned numeric value as a float, or None if the input is invalid or NaN.\n","\n","  Raises:\n","      TypeError: If the input type is not str, float, int, or None/Nan.\n","      ValueError: If the input string cannot be converted to a valid float (e.g., 'abc', '1.2.3').\n","      Exception: For other unexpected errors during cleaning number.\n","\n","  Examples:\n","      >>> extract_numeric_amount('1,500')\n","      1500.0\n","      >>> extract_numeric_amount('$1,500.99')\n","      1500.99\n","      >>> extract_numeric_amount('1500')\n","      1500.0\n","      >>> extract_numeric_amount('None')\n","      None\n","      >>> extract_numeric_amount('abc')\n","      None\n","      >>> extract_numeric_amount('9% off')\n","      9.0\n","  \"\"\"\n","  try:\n","    # Handle None, NaN, or pd.NA\n","    if number is None or pd.isna(number):\n","      return None\n","\n","    # Handle numeric inputs (int or float)\n","    if isinstance(number, float):\n","      return round(number,2)\n","    if isinstance(number, int):\n","      return float(number)\n","\n","    if isinstance(number, str):\n","      number_str = number.strip().lower()\n","      if not number_str or number_str in ('none', 'nan', ''):\n","        return None\n","\n","      # Remove non-numeric characters except dots and negative sign\n","      cleaned_number = re.sub(r'[^0-9.\\-]', '', number_str)\n","\n","      # Validate the cleaned string\n","      if (not cleaned_number\n","          or cleaned_number.count('.') > 1\n","          or cleaned_number.count('-') > 1\n","          or cleaned_number.find('-') > 0\n","          ):\n","        return None\n","\n","      return round(float(cleaned_number), 2)\n","\n","    # If other type, raise error\n","    raise TypeError(f\"Unsupported type for number: {type(number)}\")\n","\n","  except ValueError as e:\n","    logger.error(f\"Error: Cannot convert number '{number}' to float: {e}\")\n","    return None\n","\n","  except TypeError as e:\n","    logger.error(f\"Type error with number '{number}': {e}\")\n","    raise\n","\n","  except Exception as e:\n","    logger.error(f\"Error: Unexpected error while processing number '{number}': {e}\")\n","    raise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6R4Y9xYtt-bh"},"outputs":[],"source":["def generate_embeddings(texts:\n","                        Union[str, List[str]],\n","                        model: SentenceTransformer\n","                        ) -> List[List[float]]:\n","    \"\"\" Generate embeddings for a list of texts using the sentence transformer model.\n","\n","    Args:\n","        texts (Union[str, List[str]]): Input text or list of tests to encode.\n","        model (SentenceTransformer): Pretrained SentenceTransformer model.\n","\n","    Returns:\n","        List[List[float]]: List of embeddings as lists of floats.\n","\n","    Raises:\n","        Exception: For unexpected errors during embeddings.\n","    \"\"\"\n","\n","    if not texts:\n","        return []\n","\n","    # Ensure texts is a list; if a single string, convert to list\n","    if isinstance(texts, str):\n","        texts = [texts]\n","\n","    try:\n","        # Generate embeddings in batch\n","        embeddings: np.ndarray = model.encode(texts,\n","                                              show_progress_bar=True,\n","                                              convert_to_numpy=True\n","                                              )\n","        # Convert NumPy array to list for compatibility with ChromaDB\n","        return embeddings.tolist()\n","    except Exception as e:\n","        print(f\"Error generating embeddings: {e}\")\n","        raise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D81lvSmPuFpd"},"outputs":[],"source":["# Create metadata for ChromaDB\n","def create_metadata(row: pd.Series) -> Dict[str, Any]:\n","    \"\"\"Create JSON-serializable metadata dictionary.\"\"\"\n","    return {\n","        'price': row['Price'] if pd.notna(row['Price']) else None,\n","        'original_price': row['Original Price'] if pd.notna(row['Original Price']) else None,\n","        'discount': row['Discount'] if pd.notna(row['Discount']) else 0.0,\n","        'sizes': row['Sizes'] if pd.notna(row['Sizes']) else '',\n","        'product_link': row['Product Link'] if pd.notna(row['Product Link']) else '',\n","        'image_urls': row['Image URLs'] if pd.notna(row['Image URLs']) else ''\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CBiXUtWuOdL"},"outputs":[],"source":["def create_and_populate_chromadb(df):\n","    \"\"\"Create a ChromaDB collection and add data.\"\"\"\n","    client = chromadb.PersistentClient(path=\"/content/drive/MyDrive/RAG_Chatbot_System/chroma_db\")\n","    try:\n","        collection = client.get_or_create_collection(\"TNT_Store\")\n","        print(\"Collection created successfully...\")\n","\n","        # Add data to the collection\n","        collection.add(\n","            ids=[str(i) for i in range(len(df))],\n","            documents = df['text_to_embed'].tolist(),\n","            embeddings=df['embeddings'].tolist(),\n","            metadatas=df['metadata'].tolist()\n","        )\n","        print(\"Data added to collection successfully...\")\n","        return collection\n","    except Exception as e:\n","        print(f\"Error creating or populating collection: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJJ3DFdyuVt_"},"outputs":[],"source":["def format_context(results, df):\n","  context_lines = []\n","  for id_, metadata, distance in zip(results['ids'][0], results['metadatas'][0], results['distances'][0]):\n","    product_name = df.loc[int(id_), 'Title'] if int(id_) in df.index else 'N/A'\n","    description = df.loc[int(id_), 'Description'] if int(id_) in df.index else 'N/A'\n","\n","    product_info = (\n","        f\"\\n Product ID: {id_}, Distance: {distance:.4f}\\n\"\n","        f\"Product Name: {product_name}\\n\"\n","        f\"Product Link: {metadata['product_link'] or 'N/A'}\\n\"\n","        f\"Orginal Price: {metadata['original_price'] if metadata['original_price'] is not None else 'N/A'}\\n\"\n","        f\"Price: {metadata['price'] if metadata['price'] is not None else 'N/A'}\\n\"\n","        f\"Discount: {metadata['discount']}% off\\n\"\n","        f\"Sizes: {metadata['sizes'] or 'N/A'}\\n\"\n","        f\"Description: {description}\\n\"\n","    )\n","    context_lines.append(product_info)\n","  return \"\\n---\\n\".join(context_lines)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMiu3ZnFuWZ2"},"outputs":[],"source":["def generate_response(query: str,\n","                      tokenizer: AutoTokenizer,\n","                      model: AutoModelForCausalLM,\n","                      context: str = \"\",\n","                      extra: bool = False,\n","                      max_new_tokens: int = 200,\n","                      temperature: float = 0.3,\n","                      top_p: float = 0.8\n","                      ) -> str:\n","  \"\"\" Generate a response using the language model based on the query and context. \"\"\"\n","\n","  if context.strip():\n","    prompt = (\n","        f\"You are a helpful and concise shopping assistant for an online clothing store. \"\n","        f\"The user asked: '{query}'. \"\n","        f\"Here are relevant products from the catalog:\\n{context}\\n\\n\"\n","        f\"Reply briefly and naturally. \"\n","        f\"If the user asks about a product category or availability (e.g., 'Do you have sweaters?'), \"\n","        f\"respond with a short yes/no and mention some matching items. \"\n","        f\"If the user asks about a specific product, give a short description with key details like material, sizes, or price. \"\n","        f\"Only include product links if directly relevant. \"\n","        f\"Keep it friendly and to the point.\\n\\n\"\n","        f\"Response:\"\n","        )\n","\n","  elif extra:\n","    prompt = (\n","        f\"You are a helpful assistant for a clothing store.\\n\"\n","        f\"The user asked: {query}\\n\"\n","        f\"Politely respond with something like: \"\n","        f\"'Sorry, we’re unable to answer your question as it falls outside the scope of our clothing store services.'\\n\"\n","        f\"Also remind the user they can type 'exit' to end the conversation.\\n\"\n","        f\"Response:\"\n","        )\n","\n","  else:\n","    prompt = (\n","        f\"You are a helpful assistant for a clothing store.\\n\"\n","        f\"The user asked: {query}\\n\"\n","        f\"Start with a friendly greeting, ask what they're looking for, and offer help finding clothing items.\\n\"\n","        f\"Response:\"\n","        )\n","\n","  # input_text = f\"User query: {query}\\n\\nContext:\\t{context}\\n\\nAnswer:\\t\"\n","\n","  inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, return_attention_mask=True)\n","\n","  inputs = {key: value.to(model.device) for key, value in inputs.items()}\n","\n","  with torch.inference_mode():\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_new_tokens=max_new_tokens,\n","        temperature=temperature,\n","        top_p=top_p,\n","        do_sample=True,\n","        pad_token_id=tokenizer.eos_token_id,\n","        num_return_sequences=1\n","        )\n","\n","  response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","  response_start = response_text.find(\"Response:\") + len(\"Response:\")\n","  return response_text[response_start:].strip()"]},{"cell_type":"code","source":["def truncate_context(context: str, tokenizer: PreTrainedTokenizerBase, max_tokens: int = 1024) -> str:\n","  \"\"\"Truncate context to a maximum number of tokens.\"\"\"\n","  tokens = tokenizer.encode(context, truncation=True, max_length=max_tokens)\n","  return tokenizer.decode(tokens, skip_special_tokens=True)"],"metadata":{"id":"A3RnPTSZ1Xw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hash_file(filepath: str) -> str:\n","  \"\"\"Generate SHA256 hash of a file\"\"\"\n","  with open(filepath, \"rb\") as f:\n","    return hashlib.sha256(f.read()).hexdigest()"],"metadata":{"id":"mQW4yMjJA7b_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def has_csv_changed(csv_path: str, hash_path: str = \"data_hash.txt\") -> bool:\n","  current_hash = hash_file(csv_path)\n","\n","  # If hash file doesn't exist, assume it's changed\n","  if not os.path.exists(hash_path):\n","    with open(hash_path, \"w\") as f:\n","      f.write(current_hash)\n","    return True\n","\n","  with open(hash_path, \"r\") as f:\n","    previous_hash = f.read().strip()\n","\n","  # If hash differs, update and retrun True\n","  if current_hash != previous_hash:\n","    with open(hash_path, \"w\") as f:\n","      f.write(current_hash)\n","    return True\n","\n","  return False"],"metadata":{"id":"htAEHxinSPtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_email(text: str) -> Optional[str]:\n","  \"\"\"Extract a vaild email from a text string.\"\"\"\n","  match = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text)\n","  if match:\n","    return match.group(0)\n","  return None\n","\n","def is_valid_email(email: str) -> bool:\n","  return re.match(r\"[^@]+@[^@]+\\.[^@]+\", email) is not None\n","\n","def save_email_to_csv(email: str, query: str, file_path: str = \"interested_customers.csv\"):\n","  new_entry = pd.DataFrame([[email, query]], columns = [\"Email\", \"Query\"])\n","\n","  if os.path.exists(file_path):\n","    existing_data = pd.read_csv(file_path)\n","    updated_data = pd.concat([existing_data, new_entry], ignore_index=True)\n","  else:\n","    updated_data = new_entry\n","\n","  updated_data.to_csv(file_path, index=False)\n","\n","# !pip install email-validator\n","\n","# from email-validator import validate_email, EmailNotValidError\n","\n","# def is_valid_email(email: str) -> bool:\n","#   try:\n","#     validate_email(email)\n","#     return True\n","#   except EmailNotValidError:\n","#     return False"],"metadata":{"id":"Mp7AhbQJe0gp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_product_query(query: str) -> bool:\n","  keywords = [\n","      # Basic intent\n","      \"buy\", \"purchase\", \"order\", \"have\", \"sell\", \"selling\", \"looking for\", \"need\", \"want\",\n","      \"available\", \"product\", \"item\", \"in stock\", \"get\", \"shop\", \"price\", \"cost\", \"quote\",\n","      \"how much\", \"find\", \"availability\", \"stock\", \"deal\", \"offer\", \"discount\", \"checkout\",\n","      \"cart\", \"wishlist\", \"restock\", \"can I get\", \"do you have\", \"i'm after\", \"pick up\",\n","      \"deliver\", \"shipping\", \"acquire\", \"procure\",\n","\n","      # Slang & casual speak\n","      \"hook me up\", \"got some\", \"snag one\", \"cop\", \"grab one\", \"where’s it at\",\n","      \"buying vibes\", \"hit me with\", \"send me the link\", \"link to buy\"\n","  ]\n","  return any(word in query.lower() for word in keywords)"],"metadata":{"id":"aR5bsiQPpFC3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_greeting_query(query: str) -> bool:\n","    keywords = [\n","        \"hi\", \"hello\", \"hey\", \"greetings\", \"good morning\", \"good afternoon\",\n","        \"good evening\", \"howdy\", \"what's up\", \"how's it going\", \"yo\", \"sup\",\n","        \"what's good\", \"salutations\", \"hey there\", \"yo yo\", \"wassup\", \"hiya\",\n","        \"what's crackin'\", \"sup dude\", \"what's popping\", \"g'day\", \"mornin'\",\n","        \"heeeey\", \"wuddup\", \"cheers\", \"hail\", \"wazzup\", \"yello\", \"heyyy\",\n","        \"what's happening\", \"look who's here\", \"ahoy\", \"how do\", \"how's everything\",\n","        \"what’s new\", \"what’s up with you\", \"long time no see\", \"all good?\",\n","        \"what's the word\", \"what's going on\", \"good to see you\", \"hello there\",\n","        \"how are things\", \"what’s cracking\", \"what’s the haps\", \"sup fam\", \"hey hey\",\n","        \"yo, what's up\", \"how’s life\", \"what’s up with that\", \"what’s shaking\",\n","        \"how’s your day\", \"everything alright?\", \"what’s cooking\", \"howdy partner\",\n","        \"what’s the deal\", \"how goes it\", \"what’s the buzz\", \"what’s going down\",\n","        \"what’s the scoop\"\n","    ]\n","\n","    query_cleaned = re.sub(r'[^a-zA-Z\\s]', '', query).lower()\n","\n","    return any(word in query_cleaned for word in keywords)"],"metadata":{"id":"MnVtYXRncP4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Initilize the sentence transformer model\n","try:\n","  embeddingModel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n","except Exception as e:\n","  print(f\"Error loading model: {e}\")\n","  raise\n","\n","#Initilize the generation model\n","model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","generationTokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","if generationTokenizer.pad_token is None:\n","  generationTokenizer.pad_token = generationTokenizer.eos_token\n","\n","generationModel = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",  # put model on GPU automatically\n","    quantization_config=bnb_config\n","    )"],"metadata":{"id":"tcKTN8g9R6bT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"rVTU3Dj4t4DZ"},"outputs":[],"source":["csv_file = 'tnt.csv'\n","\n","pkl_path = \"embeddings.pkl\"\n","\n","# need_to_generate = (\n","#     has_csv_changed(csv_file)\n","#     or not os.path.exists(pkl_path)\n","#     or os.path.getsize(pkl_path) == 0\n","# )\n","\n","need_to_generate = True\n","\n","if need_to_generate:\n","  print(\"Generating embeddings and saving...\")\n","\n","  #Load the CSV file\n","  df = load_csv()\n","\n","  # CLean and normalize data\n","  df['Price'] = df['Price'].apply(extract_numeric_amount)\n","  df['Original Price'] = df['Original Price'].apply(extract_numeric_amount)\n","  df['Discount'] = df['Discount'].apply(extract_numeric_amount)\n","  df['Title'] = df['Title'].fillna('').str.lower().str.strip()\n","  df['Variants'] = df['Variants'].fillna('').str.lower().str.strip()\n","  df['Description'] = df['Description'].fillna('').str.lower().str.strip()\n","  df['Sizes'] = df['Sizes'].fillna('').str.lower().str.strip()\n","\n","  # Fill missing Original Price with Price\n","  df['Original Price'] = df['Original Price'].fillna(df['Price'])\n","\n","  # Create a combined text field for embedding\n","  df['text_to_embed'] = (df['Title'] + ' ' +\n","                        df['Variants'] + ' ' +\n","                        df['Description'] + ' ' +\n","                        'Sizes: ' + df['Sizes']).str.strip()\n","\n","  # Generate embeddings\n","  try:\n","    # embeddings = model.encode(df['text_to_embed'].tolist(), show_progress_bar=True)\n","    df['embeddings'] = generate_embeddings(df['text_to_embed'].tolist(), embeddingModel)\n","  except Exception as e:\n","    print(f\"Error generating embedding: {e}\")\n","    raise\n","\n","  # Create metadata\n","  df['metadata'] = df.apply(create_metadata, axis=1)\n","\n","  # Save embeddings and metadata\n","  with open(\"embeddings.pkl\", \"wb\") as f:\n","    pickle.dump(df[['embeddings', 'metadata']], f)\n","\n","  print(\"Embeddings saved.\")\n","\n","else:\n","  print(\"No CSV change: loading cached embeddings...\")\n","  df = load_csv(csv_file)\n","  with open(\"embeddings.pkl\", \"rb\") as f:\n","    loaded = pickle.load(f)\n","  df['embeddings'] = loaded['embeddings']\n","  df['metadata'] = loaded['metadata']\n"]},{"cell_type":"code","source":["collection = create_and_populate_chromadb(df)"],"metadata":{"id":"Nt0Lv0SwRWvf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8M5pVAgDuaAI"},"outputs":[],"source":["while(True):\n","  query = input(\"Hello, How can I help you:\\n\")\n","  if query.lower() == 'exit':\n","    break\n","\n","  queryEmbedding = generate_embeddings([query], embeddingModel)[0]\n","  context = collection.query(query_embeddings=[queryEmbedding], n_results=10)\n","\n","  topDistance = context['distances'][0][0]\n","  threshold = 8.0\n","\n","  if topDistance > threshold:\n","    greeting = is_greeting_query(query)\n","    unknownProduct = is_product_query(query)\n","\n","    if unknownProduct:\n","      print(\"Sorry, we couldn't find any products related to your query.\")\n","      email_input = input(\"Would you like to be notified when such products become available? Please enter your email, or type 'no' to skip:\\n\")\n","      if email_input.lower() in ['no', 'n']:\n","        print(f\"No problem. Thank you for visiting!\")\n","      else:\n","        email = extract_email(email_input)\n","        if email is not None and is_valid_email(email):\n","          save_email_to_csv(email, query)\n","          print(f\"Thank you! We'll notify you at {email} as soon as relevant products are available.\")\n","        else:\n","          print(\"That doesn't seem like a valid email address. Please try again next time.\")\n","      continue\n","\n","    elif greeting:\n","      response = generate_response(query, generationTokenizer, generationModel)\n","\n","    else:\n","      extra = True\n","      response = generate_response(query, generationTokenizer, generationModel, extra=extra)\n","\n","  else:\n","    retrievedContext = format_context(context, df)\n","    truncatedContext = truncate_context(retrievedContext, generationTokenizer, max_tokens=1024)\n","    response = generate_response(query, generationTokenizer, generationModel, context=truncatedContext)\n","\n","  print(\"Answer: \\n\")\n","  wrapped = textwrap.fill(response, width=100)\n","  print(wrapped)"]},{"cell_type":"code","source":["# # Debugging\n","# while(True):\n","#   query = input(\"What's up:\")\n","#   if query.lower() == 'exit':\n","#     break\n","#   queryEmbedding = generate_embeddings([query], embeddingModel)[0]\n","#   context = collection.query(query_embeddings=[queryEmbedding], n_results = 5)\n","#   results = context[\"documents\"]\n","#   distance = context[\"distances\"]\n","#   print(context)\n","#   print(results)\n","#   print(\"\\n\",distance)"],"metadata":{"id":"ZT2trMj1sLn0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}