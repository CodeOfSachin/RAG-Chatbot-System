{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"NzClv1NSvp0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","projectPath = '/content/drive/MyDrive/RAG_Chatbot_System'\n","os.chdir(projectPath)"],"metadata":{"id":"Lyuhdyqf7ler"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Z7Ay3ZqGtUZP"},"outputs":[],"source":["# Required packages\n","!pip install -U sentence-transformers chromadb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kF06A6MhD8MD"},"outputs":[],"source":["!pip install transformers torch huggingface-hub -qqq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCpP6Zl321Jh"},"outputs":[],"source":["!pip install -q accelerate bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUMusi96tcTQ"},"outputs":[],"source":["# Import Required packages\n","# import config  # Token Key\n","import pandas as pd\n","import numpy as np\n","import chromadb\n","import re\n","import logging\n","import torch\n","import textwrap\n","from sentence_transformers import SentenceTransformer\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedTokenizerBase\n","from typing import Dict, Any, List, Union, Optional\n","from huggingface_hub import login\n","import hashlib\n","import pickle\n","\n","login(token = config.huggingFaceToken) # Your Hugging Face Login Token\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)"]},{"cell_type":"code","source":["import spacy\n","from spacy.matcher import PhraseMatcher"],"metadata":{"id":"hA-uCuh-V5y1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9m0YQXqtwy5"},"outputs":[],"source":["# Load the CSV file\n","def load_csv(fileName: str, encoding: Optional[str] = 'utf-8') -> pd.DataFrame:\n","  \"\"\" Load a CSV file into a pandas DataFrame.\n","  Args:\n","      fileName (str, optional): Path to the CSV file. Defaults to 'tnt.csv'.\n","      encoding (str, None, optional): Encoding of the CSV file. Defaults to 'utf-8'.\n","\n","  Returns:\n","      pd.DataFrame: The loaded DataFrame.\n","\n","  Raises:\n","      FileNotFoundError: If the specified file does not exist.\n","      pd.errors.ParserError: If the CSV file is malformed.\n","      UnicodeDecodeError: If the file cannot be decoded with the specified encoding.\n","      Exception: For other unexpected errors during CSV loading.\n","  \"\"\"\n","  try:\n","    df = pd.read_csv(fileName, encoding=encoding)\n","  except FileNotFoundError:\n","    logger.error(f\"Error: File '{fileName}' not found. Please ensure the file exists in the working directory.\")\n","    raise\n","  except pd.errors.ParserError as e:\n","    logger.error(f\"Error: Failed to parse CSV file '{fileName}'. The file may be malformed: {e}\")\n","    raise\n","  except UnicodeDecodeError as e:\n","    logger.error(f\"Error: Unable to decode file '{fileName}' with encoding '{encoding}': {e}\")\n","    raise\n","  except Exception as e:\n","    logger.error(f\"Error: An unexpected error occurred while loading '{fileName}': {e}\")\n","    raise\n","  else:\n","    logger.info(f\"Successfully loaded the file: {fileName}\")\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6R4Y9xYtt-bh"},"outputs":[],"source":["def generate_embeddings(texts:\n","                        Union[str, List[str]],\n","                        model: SentenceTransformer\n","                        ) -> List[List[float]]:\n","    \"\"\" Generate embeddings for a list of texts using the sentence transformer model.\n","\n","    Args:\n","        texts (Union[str, List[str]]): Input text or list of tests to encode.\n","        model (SentenceTransformer): Pretrained SentenceTransformer model.\n","\n","    Returns:\n","        List[List[float]]: List of embeddings as lists of floats.\n","\n","    Raises:\n","        Exception: For unexpected errors during embeddings.\n","    \"\"\"\n","\n","    if not texts:\n","        return []\n","\n","    # Ensure texts is a list; if a single string, convert to list\n","    if isinstance(texts, str):\n","        texts = [texts]\n","\n","    try:\n","        # Generate embeddings in batch\n","        embeddings: np.ndarray = model.encode(texts,\n","                                              show_progress_bar=True,\n","                                              convert_to_numpy=True\n","                                              )\n","        # Convert NumPy array to list for compatibility with ChromaDB\n","        return embeddings.tolist()\n","    except Exception as e:\n","        print(f\"Error generating embeddings: {e}\")\n","        raise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMiu3ZnFuWZ2"},"outputs":[],"source":["def generate_response(query: str,\n","                      tokenizer: AutoTokenizer,\n","                      model: AutoModelForCausalLM,\n","                      context: str = \"\",\n","                      productMatched: list = \"\",\n","                      max_new_tokens: int = 200,\n","                      temperature: float = 0.3,\n","                      top_p: float = 0.8\n","                      ) -> str:\n","  \"\"\" Generate a response using the language model based on the query and context. \"\"\"\n","\n","  if productMatched:\n","    prompt = (\n","      f\"You are a helpful and concise shopping assistant for an online clothing store.\\n\"\n","      f\"The user asked: '{query}'\\n\"\n","      f\"The user is asking about this specific product: '{productMatched}'\\n\\n\"\n","      f\"Here are relevant products from the catalog (including availability status):\\n{context}\\n\\n\"\n","      f\"Reply briefly, naturally, and in a friendly tone.\\n\"\n","      f\"- If the user asks a yes/no question, respond with a short and polite yes or no.\\n\"\n","      f\"- If the user asks for product details, answer based on the product and the user’s question.\\n\"\n","      f\"- If the user asks about a product and its availability is anything other than 'in_stock' (e.g., 'out_of_stock', 'preorder', etc.), politely inform them it's not currently available and ask if the user would like to provide their email to be notified when it’s available again.\\n\"\n","      f\"Keep the response friendly, accurate, and to the point.\\n\\n\"\n","      f\"Response:\"\n","    )\n","\n","  elif context.strip():\n","    prompt = (\n","        f\"You are a helpful and concise shopping assistant for an online clothing store.\\n\"\n","        f\"The user asked: '{query}'\\n\\n\"\n","        f\"Here are relevant products from the catalog:\\n{context}\\n\\n\"\n","        f\"Reply briefly, naturally, and in a friendly tone.\\n\"\n","        f\"- If the user asks about a clothing product in general (e.g., 'T-shirt'), but not a specific product name, suggest the most relevant matching product(s) from the catalog. \"\n","        f\"For example, if they ask for 'T-shirt', suggest something like 'Summer Men T-shirt' from the product list.\\n\"\n","        f\"- If the user's question does not clearly relate to any product in the catalog, or is unclear or contains errors, politely respond with: \"\n","        f\"\\\"I'm sorry, I couldn't quite understand your request. Could you please rephrase it?\\\"\\n\"\n","        f\"Keep your response short and helpful.\\n\\n\"\n","        f\"Response:\"\n","    )\n","\n","  else:\n","    prompt = (\n","        f\"You are a helpful assistant for a clothing store.\\n\"\n","        f\"The user said: {query}\\n\\n\"\n","        f\"Respond only based on the user's input, following these rules:\\n\"\n","        f\"- If the user says 'Hi', 'Hello', or gives a greeting only, reply with a warm welcome and ask what type of clothing they're looking for. Do NOT mention products, availability, or ask for their email.\\n\"\n","        f\"- If the user asks for a specific clothing item, politely apologize for not having it and ask for their email to notify them if it becomes available.\\n\"\n","        f\"- If the user asks about something unrelated to clothing, kindly explain that your store only sells clothing and encourage them to ask about clothing instead.\\n\"\n","        f\"- Do not mention product availability or email collection unless the user clearly requested a specific item.\\n\"\n","        f\"- If the user input is unclear or doesn't match any condition, ask them politely to clarify.\\n\"\n","        f\"- Always keep your tone friendly, natural, and your response brief.\\n\"\n","        f\"- Remind the user they can type 'exit' to end the conversation.\\n\\n\"\n","        f\"Response:\"\n","    )\n","\n","  # input_text = f\"User query: {query}\\n\\nContext:\\t{context}\\n\\nAnswer:\\t\"\n","\n","  inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, return_attention_mask=True)\n","\n","  inputs = {key: value.to(model.device) for key, value in inputs.items()}\n","\n","  with torch.inference_mode():\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"],\n","        max_new_tokens=max_new_tokens,\n","        temperature=temperature,\n","        top_p=top_p,\n","        do_sample=True,\n","        pad_token_id=tokenizer.eos_token_id,\n","        num_return_sequences=1\n","        )\n","\n","  response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","  response_start = response_text.find(\"Response:\") + len(\"Response:\")\n","  return response_text[response_start:].strip()"]},{"cell_type":"code","source":["def create_document(row: pd.Series) -> Dict[str, Any]:\n","  \"\"\"Create Document for embedding and chromadb\"\"\"\n","  return (\n","      row['product_name'] + ' ' +\n","      row['description'] + ' ' +\n","      row['color'] + ' ' +\n","      row['material'] + ' ' +\n","      row['category'] + ' ' +\n","      row['brand'] + ' ' +\n","      row['audience'] + ' ' +\n","      row['tags'] + ' '\n","  )\n","\n","# Create metadata for ChromaDB\n","def create_metadata(row: pd.Series) -> Dict[str, Any]:\n","    \"\"\"Create JSON-serializable metadata dictionary.\"\"\"\n","    return {\n","        'product_id': row['product_id'] if pd.notna(row['product_id']) else None,\n","        'sizes': row['size'] if pd.notna(row['size']) else '',\n","        'image_url': row['image_url'] if pd.notna(row['image_url']) else '',\n","        'price': row['price'] if pd.notna(row['price']) else None,\n","        'discount_price': row['discount_price'] if pd.notna(row['discount_price']) else None,\n","        'discount_percent': row['discount_percent'] if pd.notna(row['discount_percent']) else 0.0,\n","        'stock_quantity': row['stock_quantity'] if pd.notna(row['stock_quantity']) else 0,\n","        'created_at': row['created_at'] if pd.notna(row['created_at']) else None\n","    }"],"metadata":{"id":"PkKe4JgB4dW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CBiXUtWuOdL"},"outputs":[],"source":["def create_and_populate_chromadb(df):\n","    \"\"\"Create a ChromaDB collection and add data.\"\"\"\n","    client = chromadb.PersistentClient(path=\"/content/drive/MyDrive/RAG_Chatbot_System/chroma_db\")\n","    try:\n","        collection = client.get_or_create_collection(\"TNT_Store\")\n","        print(\"Collection created successfully...\")\n","\n","        # Add data to the collection\n","        collection.add(\n","            ids=[str(i) for i in range(len(df))],\n","            documents = df['document'].tolist(),\n","            embeddings=df['embeddings'].tolist(),\n","            metadatas=df['metadata'].tolist()\n","        )\n","        print(\"Data added to collection successfully...\")\n","        return collection\n","    except Exception as e:\n","        print(f\"Error creating or populating collection: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJJ3DFdyuVt_"},"outputs":[],"source":["def format_context(results, df):\n","  context_lines = []\n","\n","  for id, distance in zip(results['ids'][0], results['distances'][0]):\n","    product_id = df.loc[int(id), 'product_id'] if int(id) in df.index else 'N/A'\n","    product_name = df.loc[int(id), 'product_name'] if int(id) in df.index else 'N/A'\n","    description = df.loc[int(id), 'description'] if int(id) in df.index else 'N/A'\n","    color = df.loc[int(id), 'color'] if int(id) in df.index else 'N/A'\n","    size = df.loc[int(id), 'size'] if int(id) in df.index else 'N/A'\n","    material = df.loc[int(id), 'material'] if int(id) in df.index else 'N/A'\n","    image_url = df.loc[int(id), 'image_url'] if int(id) in df.index else 'N/A'\n","    price = df.loc[int(id), 'price'] if int(id) in df.index else 'N/A'\n","    discount_price = df.loc[int(id), 'discount_price'] if int(id) in df.index else 'N/A'\n","    discount_percent = df.loc[int(id), 'discount_percent'] if int(id) in df.index else 'N/A'\n","    category = df.loc[int(id), 'category'] if int(id) in df.index else 'N/A'\n","    brand = df.loc[int(id), 'brand'] if int(id) in df.index else 'N/A'\n","    stock_quantity = df.loc[int(id), 'stock_quantity'] if int(id) in df.index else 'N/A'\n","    audience = df.loc[int(id), 'audience'] if int(id) in df.index else 'N/A'\n","    tags = df.loc[int(id), 'tags'] if int(id) in df.index else 'N/A'\n","    created_at = df.loc[int(id), 'created_at'] if int(id) in df.index else 'N/A'\n","    availability_status = df.loc[int(id), 'availability_status'] if int(id) in df.index else 'N/A'\n","\n","    product_info = (\n","        f\"Details of product:\\n\"\n","        f\"ID: {product_id} \\n\"\n","        f\"Name: {product_name} \\n\"\n","        f\"Description: {description} \\n\"\n","        f\"Color: {color} \\n\"\n","        f\"Size: {size} \\n\"\n","        f\"Material: {material} \\n\"\n","        f\"Image: {image_url} \\n\"\n","        f\"Price: {price} \\n\"\n","        f\"Discounted Price: {discount_price} ({discount_percent}% off) \\n\"\n","        f\"Category: {category} \\n\"\n","        f\"Brand: {brand} \\n\"\n","        f\"In Stock: {stock_quantity} items\\n\"\n","        f\"For: {audience} \\n\"\n","        f\"Tags: {tags} \\n\"\n","        f\"Added on: {created_at} \\n\"\n","        f\"Availability: {availability_status} \\n\"\n","    )\n","    context_lines.append(product_info)\n","  return \"\\n---\\n\".join(context_lines)"]},{"cell_type":"code","source":["def dict_based_matcher(df, text):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n","  pattern = [nlp.make_doc(name) for name in df['product_name'].dropna()]\n","  matcher.add(\"PRODUCT\", pattern)\n","  doc = nlp(text)\n","  matches = matcher(doc)\n","  found = [doc[start:end].text for _, start, end in matches]\n","  if found:\n","    return found\n","  else:\n","    return ''"],"metadata":{"id":"C7sxkoNWWEgl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def truncate_context(context: str, tokenizer: PreTrainedTokenizerBase, max_tokens: int = 1024) -> str:\n","  \"\"\"Truncate context to a maximum number of tokens.\"\"\"\n","  tokens = tokenizer.encode(context, truncation=True, max_length=max_tokens)\n","  return tokenizer.decode(tokens, skip_special_tokens=True)"],"metadata":{"id":"A3RnPTSZ1Xw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hash_file(filepath: str) -> str:\n","  \"\"\"Generate SHA256 hash of a file\"\"\"\n","  with open(filepath, \"rb\") as f:\n","    return hashlib.sha256(f.read()).hexdigest()"],"metadata":{"id":"mQW4yMjJA7b_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def has_csv_changed(csv_path: str, hash_path: str = \"data_hash.txt\") -> bool:\n","  current_hash = hash_file(csv_path)\n","\n","  # If hash file doesn't exist, assume it's changed\n","  if not os.path.exists(hash_path):\n","    with open(hash_path, \"w\") as f:\n","      f.write(current_hash)\n","    return True\n","\n","  with open(hash_path, \"r\") as f:\n","    previous_hash = f.read().strip()\n","\n","  # If hash differs, update and retrun True\n","  if current_hash != previous_hash:\n","    with open(hash_path, \"w\") as f:\n","      f.write(current_hash)\n","    return True\n","\n","  return False"],"metadata":{"id":"htAEHxinSPtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_email(text: str) -> Optional[str]:\n","  \"\"\"Extract a vaild email from a text string.\"\"\"\n","  match = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text)\n","  if match:\n","    return match.group(0)\n","  return None\n","\n","\n","def is_valid_email(email: str) -> bool:\n","  return re.match(r\"[^@]+@[^@]+\\.[^@]+\", email) is not None\n","\n","\n","def save_email_to_csv(email: str, query: str, file_path: str = \"interested_customers.csv\"):\n","  query = query.strip().lower()\n","  email = email.strip().lower()\n","  new_entry = pd.DataFrame([[email, query]], columns = [\"Email\", \"Query\"])\n","\n","  if os.path.exists(file_path):\n","    df = pd.read_csv(file_path)\n","    updated_data = df\n","    if email in df[\"Email\"].values:\n","      existing_queries = df.loc[df[\"Email\"] == email, \"Query\"].values[0]\n","      existing_list = [q.strip().lower() for q in existing_queries.split(',')]\n","      if query not in existing_list:\n","        df.loc[df[\"Email\"] == email, \"Query\"] = existing_queries + ', ' + query\n","        updated_data = df\n","    else:\n","      updated_data = pd.concat([df, new_entry], ignore_index=True)\n","  else:\n","    updated_data = new_entry\n","\n","  updated_data.to_csv(file_path, index=False)\n","\n","\n","# !pip install email-validator\n","\n","# from email-validator import validate_email, EmailNotValidError\n","\n","# def is_valid_email(email: str) -> bool:\n","#   try:\n","#     validate_email(email)\n","#     return True\n","#   except EmailNotValidError:\n","#     return False"],"metadata":{"id":"Mp7AhbQJe0gp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Initilize the sentence transformer model\n","try:\n","  embeddingModel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n","except Exception as e:\n","  print(f\"Error loading model: {e}\")\n","  raise\n","\n","#Initilize the generation model\n","model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","generationTokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","if generationTokenizer.pad_token is None:\n","  generationTokenizer.pad_token = generationTokenizer.eos_token\n","\n","generationModel = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",  # put model on GPU automatically\n","    quantization_config=bnb_config\n","    )"],"metadata":{"id":"tcKTN8g9R6bT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"rVTU3Dj4t4DZ"},"outputs":[],"source":["csv_file = 'sample_data.csv'\n","\n","pkl_path = \"embeddings.pkl\"\n","\n","need_to_generate = (\n","    has_csv_changed(csv_file)\n","    or not os.path.exists(pkl_path)\n","    or os.path.getsize(pkl_path) == 0\n",")\n","\n","if need_to_generate:\n","  print(\"Generating embeddings and saving...\")\n","\n","  #Load the CSV file\n","  df = load_csv(csv_file)\n","\n","  # Create document\n","  df['document'] = df.apply(create_document, axis=1)\n","\n","  # Generate embeddings\n","  try:\n","    # embeddings = model.encode(df['text_to_embed'].tolist(), show_progress_bar=True)\n","    df['embeddings'] = generate_embeddings(df['document'].tolist(), embeddingModel)\n","  except Exception as e:\n","    print(f\"Error generating embedding: {e}\")\n","    raise\n","\n","  # Create metadata\n","  df['metadata'] = df.apply(create_metadata, axis=1)\n","\n","  # Save embeddings and metadata\n","  with open(\"embeddings.pkl\", \"wb\") as f:\n","    pickle.dump(df[['embeddings', 'metadata', 'document']], f)\n","\n","  print(\"Embeddings saved.\")\n","\n","else:\n","  print(\"No CSV change: loading cached embeddings...\")\n","  df = load_csv(csv_file)\n","  with open(\"embeddings.pkl\", \"rb\") as f:\n","    loaded = pickle.load(f)\n","  df['embeddings'] = loaded['embeddings']\n","  df['metadata'] = loaded['metadata']\n","  df['document'] = loaded['document']\n"]},{"cell_type":"code","source":["collection = create_and_populate_chromadb(df)"],"metadata":{"id":"KW1G-v2PA7x6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8M5pVAgDuaAI"},"outputs":[],"source":["print(\"Hello, How can I help you:\\n\")\n","print(\"Type 'exit' to end the chat.\\n\")\n","chatHistory = []\n","numberOfChat = 0\n","lastQuery = None\n","while(True):\n","  try:\n","    query = input().strip()\n","  except Exception as e:\n","    print(f\"Input Error: {e}\")\n","    continue\n","\n","  if query.lower() == 'exit':\n","    break\n","\n","  numberOfChat += 1\n","\n","  if numberOfChat > 1:\n","    email = extract_email(query)\n","    if email is not None and is_valid_email(email):\n","      save_email_to_csv(email, lastQuery)\n","      print(f\"Thank you! We'll notify you at {email} as soon as relevant products are available.\")\n","      continue\n","\n","  productNameMatcher = dict_based_matcher(df, query)\n","  if productNameMatcher:\n","    productNameEmbedding = generate_embeddings(productNameMatcher, embeddingModel)[0]\n","    context = collection.query(query_embeddings=[productNameEmbedding], n_results=5)\n","    retrievedContext = format_context(context, df)\n","    truncatedContext = truncate_context(retrievedContext, generationTokenizer, max_tokens=1024)\n","    response = generate_response(query, generationTokenizer, generationModel, context=truncatedContext, productMatched=productNameMatcher)\n","\n","  else:\n","    queryEmbedding = generate_embeddings([query], embeddingModel)[0]\n","    context = collection.query(query_embeddings=[queryEmbedding], n_results=10)\n","\n","    topDistance = context['distances'][0][0]\n","\n","    thresholdDistance = 10.0 # Threshold for semantic similarity; lower is more similar\n","\n","    if topDistance > thresholdDistance:\n","      response = generate_response(query, generationTokenizer, generationModel)\n","\n","    else:\n","      retrievedContext = format_context(context, df)\n","      truncatedContext = truncate_context(retrievedContext, generationTokenizer, max_tokens=1024)\n","      response = generate_response(query, generationTokenizer, generationModel, context=truncatedContext)\n","\n","  lastQuery = query\n","  chat = {f\"Query {numberOfChat}\": query, f\"Response {numberOfChat}\": response}\n","  chatHistory.append(chat)\n","  print(\"Answer: \\n\")\n","  wrapped = textwrap.fill(response, width=100)\n","  print(wrapped)"]},{"cell_type":"code","source":["# # Debugging\n","# while(True):\n","#   query = input(\"What's up:\")\n","#   if query.lower() == 'exit':\n","#     break\n","#   queryEmbedding = generate_embeddings([query], embeddingModel)[0]\n","#   context = collection.query(query_embeddings=[queryEmbedding], n_results = 5)\n","#   results = context[\"documents\"]\n","#   distance = context[\"distances\"]\n","#   print(context)\n","#   print(results)\n","#   print(\"\\n\",distance)"],"metadata":{"id":"ZT2trMj1sLn0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}